{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "An online book store company, Valdivian,  wants to analyse the data they've been collecting on books and user ratings of these specific books from their app and online website. The analytics team is particularly interested in understanding which books users rate the most, and the data scientists are interested in using machine learning algorithms to develop a book recommendation system. Currently, they don't have an easy way to query their data, which resides in a directories containing a CSV file on user data, JSON logs of user ratings and another CSV file of metadata on books.\n",
    "\n",
    "They want to implement a data warehouse which is designed to optimise queries on user rating analysis, additionally, the ETL pipelines are to be high-grade data pipelines that:\n",
    "* Are automated and easily monitored. \n",
    "* Are dynamic and built from reusable tasks.\n",
    "* Have automated data quality checks that catch any discrepancies in the datasets.\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from time import time\n",
    "import psycopg2\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The source data resides on AWS S3 and needs to be processed in Sparkify's data warehouse which resides on Amazon Redshift. The source datasets consist of JSON file of logs that tell them about users **ratings** of books on their store, CSV files that hold **users** information, and CSV metadata about the **books** the are available on the store.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The original datasets (BX-Book-Ratings, BX-Books, BX-Users) are from https://www.kaggle.com/ruchi798/bookcrossing-dataset. BX-Book-Ratings will be converted to .JSON format and renamed to **ratings**, while the other two datasets respectively will be renamed to **books** and **users**. Subsequently they will be saved on my S3 bucket. For details of the bucket, folder structure and row 0 of the dataset please refer to the below:\n",
    "\n",
    "> s3://udacity-capstone-project-828/ratings: data about book ratings, example of row '0' of the data as follows: {'Book-Rating': 0, 'ISBN': '034545104X', 'User-ID': 276725}\n",
    "\n",
    "> s3://udacity-capstone-project-828/users: data about users, example of row '0' of the data as follows: {'User-ID': 1, 'Location': 'nyc, new york, usa', 'Age': nan}\n",
    "\n",
    "> s3://udacity-capstone-project-828/books: data books available from this store, example of row '0' of the data as follows: {'ISBN': '0195153448', 'book_title': 'Classical Mythology', 'book_author': 'Mark P. O. Morford', 'year_of_publication': '2002', 'publisher': 'Oxford University Press', 'img_s': 'http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg', 'img_m': 'http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg', 'img_l': 'http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-ID': 1, 'Location': 'nyc, new york, usa', 'Age': nan}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data here\n",
    "df_users = pd.read_csv('BX-Users.csv')\n",
    "df_users.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Book-Rating': 0, 'ISBN': '034545104X', 'User-ID': 276725}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings = pd.read_json(\"ratings.json\")\n",
    "df_ratings.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ISBN': '0195153448',\n",
       " 'book_title': 'Classical Mythology',\n",
       " 'book_author': 'Mark P. O. Morford',\n",
       " 'year_of_publication': '2002',\n",
       " 'publisher': 'Oxford University Press',\n",
       " 'img_s': 'http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg',\n",
       " 'img_m': 'http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg',\n",
       " 'img_l': 'http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books = pd.read_csv(\"BX-Books.csv\", encoding='latin-1',low_memory=False)\n",
    "df_books.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fill in NaN values\n",
    "df_users['Age'] = df_users['Age'].fillna(-1)\n",
    "df_books = df_books.fillna(\"No Data\")\n",
    "\n",
    "# Replace &amp! with \"And\"\n",
    "df_books=df_books.assign(book_author=df_books[\"book_author\"].str.replace('&amp;', 'and'))\n",
    "df_books=df_books.assign(book_title=df_books[\"book_title\"].str.replace('&amp;', 'and'))\n",
    "df_books=df_books.assign(publisher=df_books[\"publisher\"].str.replace('&amp;', 'and'))\n",
    "\n",
    "# Remove columns that are not numeric\n",
    "df_users=df_users.loc[df_users['User-ID'].astype(str).str.isnumeric()]\n",
    "df_books=df_books.loc[df_books['year_of_publication'].str.isnumeric()]\n",
    "\n",
    "# Remove columns that are not alpha numeric\n",
    "df_ratings=df_ratings.loc[df_ratings[\"ISBN\"].str.isalnum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Writing Files For Project\n",
    "Write files here to be uploaded onto S3 for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "rowmultiple=len(df_ratings)/20\n",
    "Start=0\n",
    "Finish=10\n",
    "i=0\n",
    "for i in range(0,20):\n",
    "    if i == 0:\n",
    "        Start = i*rowmultiple\n",
    "    elif i > 0:\n",
    "        Start = (i*rowmultiple)+1\n",
    "    Finish = (i+1)*rowmultiple\n",
    "    batch = df_ratings[int(Start):int(Finish)].copy()\n",
    "    batch = batch.to_json(orient='records')\n",
    "    batch = batch.strip('[]').replace('},', '}').replace('\\\\', \"\") # Make file compatible with Redshift\n",
    "    \n",
    "    with open('data/ratingsbatch{}.json'.format(i+1), 'w') as file:\n",
    "        file.write(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write to .csv\n",
    "df_users.to_csv(\"users.csv\", encoding='utf-8-sig', sep=';', index=False)\n",
    "df_books.to_csv(\"books.csv\", encoding='utf-8-sig', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Refer to Section 1 of the README on https://github.com/manchhui/Udacity-DENG-Capstone\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Refer to Section 1 of the README on https://github.com/manchhui/Udacity-DENG-Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "## STEP 1: Get the params and create staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh_crshc.cfg'))\n",
    "KEY=config.get('AWS','key')\n",
    "SECRET= config.get('AWS','secret')\n",
    "\n",
    "DWH_DB= config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER= config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD= config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_ENDPOINT= config.get(\"DWH\",\"DWH_ENDPOINT\")\n",
    "DWH_ROLE_ARN= config.get(\"IAM_ROLE\",\"ARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<connection object at 0x7f84c4881898; dsn: 'user=dwhuser password=xxx dbname=dwh host=dwhcluster.ckoszxyq4rgx.us-west-2.redshift.amazonaws.com port=5439', closed: 0>\n",
      "CPU times: user 7.08 ms, sys: 223 µs, total: 7.31 ms\n",
      "Wall time: 525 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(DWH_ENDPOINT, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT))\n",
    "cur = conn.cursor()\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.05 ms, sys: 76 µs, total: 1.13 ms\n",
      "Wall time: 289 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cur.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS \"staging_users\"; \n",
    "DROP TABLE IF EXISTS \"staging_books\"; \n",
    "DROP TABLE IF EXISTS \"staging_ratings\"; \n",
    "CREATE TABLE \"staging_users\" (\"iduser\" int,\n",
    "                            \"location\" varchar(256),\n",
    "                            \"age\" real);\n",
    "\n",
    "CREATE TABLE \"staging_books\" (\"isbn\" varchar,\n",
    "                            \"booktitle\" varchar(512),\n",
    "                            \"bookauthor\" varchar(256),\n",
    "                            \"yearofpub\" int,\n",
    "                            \"publisher\" varchar,\n",
    "                            \"img_s\" varchar(512),\n",
    "                            \"img_m\" varchar(512),\n",
    "                            \"img_l\" varchar(512));\n",
    "\n",
    "CREATE TABLE \"staging_ratings\" (\"bookrating\" varchar,\n",
    "                                \"isbn\" varchar,\n",
    "                                \"iduser\" varchar);\n",
    "\"\"\");\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## STEP 2: Copy S3 Data To Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 ms, sys: 0 ns, total: 1.33 ms\n",
      "Wall time: 2.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cur.execute(\"\"\"\n",
    "copy staging_users \n",
    "from 's3://udacity-capstone-project-828/users' \n",
    "credentials 'aws_iam_role={}'\n",
    "format as csv\n",
    "delimiter ';'\n",
    "compupdate off\n",
    "emptyasnull\n",
    "blanksasnull\n",
    "IGNOREHEADER 1\n",
    "region 'us-west-2';\n",
    "\"\"\".format(DWH_ROLE_ARN))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 ms, sys: 0 ns, total: 1.84 ms\n",
      "Wall time: 3.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cur.execute(\"\"\"\n",
    "copy staging_books \n",
    "from 's3://udacity-capstone-project-828/books' \n",
    "credentials 'aws_iam_role={}'\n",
    "format as csv\n",
    "delimiter ';'\n",
    "compupdate off\n",
    "emptyasnull\n",
    "blanksasnull\n",
    "IGNOREHEADER 1\n",
    "region 'us-west-2';\n",
    "\"\"\".format(DWH_ROLE_ARN))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 ms, sys: 0 ns, total: 1.4 ms\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cur.execute(\"\"\"\n",
    "copy staging_ratings \n",
    "from 's3://udacity-capstone-project-828/ratings' \n",
    "credentials 'aws_iam_role={}'\n",
    "format as json 's3://udacity-capstone-project-828/jsonpaths.json'\n",
    "compupdate off\n",
    "emptyasnull\n",
    "blanksasnull\n",
    "region 'us-west-2';\n",
    "\"\"\".format(DWH_ROLE_ARN))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"DELETE FROM staging_users WHERE iduser IS NULL;\"\"\")\n",
    "cur.execute(\"\"\"DELETE FROM staging_books WHERE isbn IS NULL;\"\"\")\n",
    "cur.execute(\"\"\"DELETE FROM staging_ratings WHERE bookrating IS NULL;\"\"\")\n",
    "cur.execute(\"\"\"DELETE FROM staging_ratings WHERE isbn IS NULL;\"\"\")\n",
    "cur.execute(\"\"\"DELETE FROM staging_ratings WHERE iduser IS NULL;\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## STEP 3: Create Tables for Star Schema DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 934 µs, sys: 0 ns, total: 934 µs\n",
      "Wall time: 324 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cur.execute(\"\"\"\n",
    "-- -----------------------------------------------------\n",
    "-- Table `users`\n",
    "-- -----------------------------------------------------\n",
    "DROP TABLE IF EXISTS books_publisher ;\n",
    "DROP TABLE IF EXISTS books_author ;\n",
    "DROP TABLE IF EXISTS books_title ;\n",
    "DROP TABLE IF EXISTS publishers ;\n",
    "DROP TABLE IF EXISTS authors ;\n",
    "DROP TABLE IF EXISTS titles ;\n",
    "DROP TABLE IF EXISTS yearofpub ;\n",
    "DROP TABLE IF EXISTS ratings ;\n",
    "DROP TABLE IF EXISTS users ;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "  iduser INT NOT NULL,\n",
    "  Location VARCHAR(256) NOT NULL,\n",
    "  Age INT NOT NULL,\n",
    "  PRIMARY KEY (iduser));\n",
    "  \n",
    "CREATE TABLE IF NOT EXISTS ratings (\n",
    "  idbookratings VARCHAR(256) NOT NULL,\n",
    "  ISBN VARCHAR(256) NOT NULL,\n",
    "  iduser INT NOT NULL,\n",
    "  rating INT NOT NULL,\n",
    "  PRIMARY KEY (idbookratings),\n",
    "  FOREIGN KEY (iduser) REFERENCES users (iduser));\n",
    "  \n",
    "CREATE TABLE IF NOT EXISTS yearofpub (\n",
    "  yearofpub INT NOT NULL,\n",
    "  ISBN VARCHAR(256) NOT NULL,\n",
    "  PRIMARY KEY (yearofpub),\n",
    "  FOREIGN KEY (ISBN) REFERENCES ratings (idbookratings));\n",
    "  \n",
    "CREATE TABLE IF NOT EXISTS authors (\n",
    "  idauthor VARCHAR(256) NOT NULL,\n",
    "  authorname VARCHAR(256) NOT NULL,\n",
    "  PRIMARY KEY (idauthor));\n",
    "  \n",
    "CREATE TABLE IF NOT EXISTS books_author (\n",
    "  ISBN VARCHAR(256) NOT NULL,\n",
    "  idauthor VARCHAR(256) NOT NULL,\n",
    "  PRIMARY KEY (ISBN),\n",
    "  FOREIGN KEY (idauthor) REFERENCES authors (idauthor),\n",
    "  FOREIGN KEY (ISBN) REFERENCES ratings (idbookratings));\n",
    "  \n",
    "CREATE TABLE IF NOT EXISTS titles (\n",
    "  idTitles VARCHAR(256) NOT NULL,\n",
    "  title VARCHAR(512) NOT NULL,\n",
    "  PRIMARY KEY (idTitles));\n",
    "  \n",
    "CREATE TABLE IF NOT EXISTS books_title (\n",
    "  idTitles VARCHAR(256) NOT NULL,\n",
    "  ISBN VARCHAR(256) NOT NULL,\n",
    "  PRIMARY KEY (idTitles),\n",
    "  FOREIGN KEY (idTitles) REFERENCES titles (idTitles),\n",
    "  FOREIGN KEY (ISBN) REFERENCES ratings (idbookratings));\n",
    "  \n",
    "CREATE TABLE IF NOT EXISTS publishers (\n",
    "  idPublishers VARCHAR(256) NOT NULL,\n",
    "  publisher VARCHAR(256) NOT NULL,\n",
    "  PRIMARY KEY (idPublishers));\n",
    "  \n",
    "CREATE TABLE IF NOT EXISTS books_publisher (\n",
    "  idPublishers VARCHAR(256) NOT NULL,\n",
    "  ISBN VARCHAR(256) NOT NULL,\n",
    "  PRIMARY KEY (idPublishers),\n",
    "  FOREIGN KEY (idPublishers) REFERENCES publishers (idPublishers),\n",
    "  FOREIGN KEY (ISBN) REFERENCES ratings (idbookratings));\n",
    "\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 715 µs, sys: 0 ns, total: 715 µs\n",
      "Wall time: 4.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cur.execute(\"\"\"\n",
    "-- ---------\n",
    "-- INSERT\n",
    "-- ---------\n",
    "INSERT INTO users\n",
    "SELECT distinct iduser, location, age\n",
    "FROM staging_users;\n",
    "\n",
    "CREATE TEMP TABLE ratings_staging (LIKE ratings);\n",
    "\n",
    "INSERT INTO ratings_staging\n",
    "SELECT distinct\n",
    "    isbn AS idbookratings,\n",
    "    isbn AS ISBN, \n",
    "    CAST(iduser AS INT), \n",
    "    CAST(bookrating AS INT) AS rating\n",
    "FROM staging_ratings ;\n",
    "\n",
    "INSERT INTO ratings \n",
    "SELECT distinct\n",
    "    md5(rs.ISBN || rs.iduser || rs.rating) AS idbookratings, \n",
    "    rs.ISBN, \n",
    "    rs.iduser, \n",
    "    rs.rating \n",
    "FROM (SELECT rs.* \n",
    "    FROM ratings_staging rs) rs\n",
    "LEFT JOIN ratings r\n",
    "    ON rs.idbookratings = r.idbookratings\n",
    "    WHERE r.idbookratings IS NULL;\n",
    "    \n",
    "DROP TABLE IF EXISTS ratings_staging;\n",
    "\n",
    "INSERT INTO yearofpub\n",
    "SELECT distinct yearofpub, ISBN\n",
    "FROM staging_books;\n",
    "\n",
    "INSERT INTO authors\n",
    "SELECT distinct md5(bookauthor) AS idauthor, bookauthor AS authorname\n",
    "FROM staging_books;\n",
    "\n",
    "INSERT INTO books_author\n",
    "SELECT distinct ISBN, md5(bookauthor) AS idauthor\n",
    "FROM staging_books;\n",
    "\n",
    "INSERT INTO titles\n",
    "SELECT distinct md5(booktitle) AS idTitles, booktitle AS title\n",
    "FROM staging_books;\n",
    "\n",
    "INSERT INTO books_title\n",
    "SELECT distinct md5(booktitle) AS idTitles, ISBN\n",
    "FROM staging_books;\n",
    "\n",
    "INSERT INTO publishers\n",
    "SELECT distinct md5(publisher) AS idPublishers, publisher\n",
    "FROM staging_books;\n",
    "\n",
    "INSERT INTO books_publisher\n",
    "SELECT distinct md5(publisher) AS idPublishers, ISBN\n",
    "FROM staging_books;\n",
    "\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality on table 'books_publisher' check passed with '271357' records\n",
      "Data quality on table 'books_author' check passed with '271357' records\n",
      "Data quality on table 'books_title' check passed with '271357' records\n",
      "Data quality on table 'publishers' check passed with '16760' records\n",
      "Data quality on table 'authors' check passed with '102017' records\n",
      "Data quality on table 'titles' check passed with '242012' records\n",
      "Data quality on table 'yearofpub' check passed with '271357' records\n",
      "Data quality on table 'ratings' check passed with '1148594' records\n",
      "Data quality on table 'users' check passed with '278858' records\n",
      "CPU times: user 7.16 ms, sys: 0 ns, total: 7.16 ms\n",
      "Wall time: 820 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tables =[\"books_publisher\", \"books_author\", \"books_title\", \"publishers\", \"authors\", \n",
    "         \"titles\", \"yearofpub\", \"ratings\", \"users\"]\n",
    "\n",
    "try:\n",
    "    for table in tables:\n",
    "        sql = \"select COUNT (*) from {}\".format(table)\n",
    "        number = cur.execute(sql)\n",
    "        num_records=cur.fetchone()\n",
    "        if num_records[0] > 0:\n",
    "            print(\"Data quality on table '{}' check passed with '{}' records\".format(table, num_records[0]))\n",
    "        else:\n",
    "            print(\"No records present in destination table '{}'\".format(table))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0 '{'idpublishers': '568d044227b9a875d6534dc9700207be', 'isbn': '3423241489'}' of Table 'books_publisher'\n",
      "Row 0 '{'isbn': '0345369068', 'idauthor': 'e009561f055e0ad994611c279fbd63c1'}' of Table 'books_author'\n",
      "Row 0 '{'idtitles': '6dd1bc9445f9c3c3fff565985b15b57c', 'isbn': '0312148267'}' of Table 'books_title'\n",
      "Row 0 '{'idpublishers': 'cf0da27dc129b2a35fcc046924a8e372', 'publisher': 'Laia'}' of Table 'publishers'\n",
      "Row 0 '{'idauthor': '1621320cf5f525d3d80d47a83ca58cda', 'authorname': 'David Adams Richards'}' of Table 'authors'\n",
      "Row 0 '{'idtitles': 'a9bfb64da9505aa56035a88752534753', 'title': 'Clara Callan'}' of Table 'titles'\n",
      "Row 0 '{'yearofpub': 2001, 'isbn': '0002005018'}' of Table 'yearofpub'\n",
      "Row 0 '{'idbookratings': '34f92fddc04e5d7800d21bf62c9e8d9f', 'isbn': '0373165897', 'iduser': 153662, 'rating': 0}' of Table 'ratings'\n",
      "Row 0 '{'iduser': 384, 'location': 'bucuresti, n/a, romania', 'age': 23}' of Table 'users'\n",
      "CPU times: user 151 ms, sys: 419 µs, total: 151 ms\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tables =[\"books_publisher\", \"books_author\", \"books_title\", \"publishers\", \"authors\", \n",
    "         \"titles\", \"yearofpub\", \"ratings\", \"users\"]\n",
    "\n",
    "try:\n",
    "    for table in tables:\n",
    "        sql = \"select * from {} limit 1\".format(table)\n",
    "        dat = sqlio.read_sql_query(sql, conn)\n",
    "        row=dat.iloc[0].to_dict()\n",
    "        print(\"Row 0 '{}' of Table '{}'\".format(row, table))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Visit Section 1.2 https://github.com/manchhui/Udacity-DENG-Capstone for the data dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Project Technologies & Rationale\n",
    "A brief description of each of the three core technologies that is used in this project and the rationale behind each choice:\n",
    "> **Description**: Amazon Simple Storage Service is storage for the Internet. It is designed to make web-scale computing easier for developers. **Amazon S3** has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. [aws.amazon.com](https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html). \n",
    "\n",
    "> **Description**: **Amazon Redshift** is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers. [aws.amazon.com](https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html). \n",
    "\n",
    "> **Description**: **Apache Airflow** is an open-source workflow management platform. It started at Airbnb in October 2014 as a solution to manage the company's increasingly complex workflows. Creating Airflow allowed Airbnb to programmatically author and schedule their workflows and monitor them via the built-in Airflow user interface. From the beginning, the project was made open source, becoming an Apache Incubator project in March 2016 and a Top-Level Apache Software Foundation project in January 2019. [Wikipedia](https://en.wikipedia.org/wiki/Apache_Airflow).\n",
    "\n",
    "> **Rationale**: With a potential for 100x increase in data and hundreds of concurrent users querying the data, from the current approximately one million rows or a few hundred megabytes of data and tens of users, Amazon Redshift coupled with S3 storage and Airflow is the preferred solution going forward. The technology itself is easily scalable removing the need for Valdivian to manage onsite infrastructure, amd therefore can quickly scale up or down computational resources and allow robust ETL pipelines to be built and maintained with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Proposed Data Refresh Rate\n",
    "Currently the airflow schedule has been set to refresh the data every morning at 7am, which is considered an acceptable schedule considering the need for upto data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
